# Project: Fine-Tuning Mistral 7B Model for Python Code Problem Solving

This project aims to implement state-of-the-art fine-tuning techniques on the Mistral 7B model for tasks related to Python code problem solving and debugging. The goal is to explore and apply advanced methods to enhance the performance of the Mistral 7B model on specific programming and NLP tasks.

## Author
- **Ayman Moumen**

Final-year student at Centrale Supélec, specializing in Artificial Intelligence and Data Science.

#### This project is part of the Deep Learning course at Centrale Supélec.


## Description

The Mistral 7B model is an optimized pre-trained generative language model that has demonstrated impressive capabilities across various natural language processing domains even when compared to larger models. However, adapting it to specific tasks such as Python code problem solving requires a specialized fine-tuning process.

In this project, we will explore the following techniques for fine-tuning the Mistral 7B model:

- **Task-Specific Fine-Tuning**: Adapting the model for Python code problem solving using task-specific data.
  
- **Hyperparameter Optimization**: Searching and tuning model hyperparameters to maximize performance.

- **Model Evaluation and Comparison**: Using appropriate metrics to evaluate the fine-tuned model's performance and comparing it with other baseline models.

We used advanced fine-tuning techniques such as PEFT (Parameter Efficient for Fine-Tuning) and LoRA (Low Rank Adaptation) to tailor the Mistral 7B model for Python code problem-solving tasks.

## Objectives

- Develop advanced skills in fine-tuning language models for specific tasks.
  
- Explore cutting-edge methods for solving Python code problems using pre-trained language models.

- Understand the challenges and best practices associated with fine-tuning language models for specific applications.

## Project Contents

- **Fine-Tuning Notebooks**: Contains Jupyter notebooks describing the process of fine-tuning the Mistral 7B model on Python code problem-solving tasks.

- **Training and Evaluation Data**: Datasets used for training, validation, and evaluation of the fine-tuned model. The datasets are available in the Hugging Face repository [here](https://huggingface.co/datasets/ayman56/stackoverflow_qa_python_Preprocessed).

- **Results and Analysis**: Analysis of results obtained after fine-tuning and comparison with other approaches.

## How to Use the Project

1. Clone the repository to your local machine:
   ```bash
   git clone https://github.com/Ayman2G/Fine-Tuning-Mistral-7B.git
